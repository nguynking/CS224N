\graphicspath{ {images/} }

\titledquestion{Analyzing NMT Systems}[25]

\begin{parts}

    \part[3] Look at the {\monofam{src.vocab}} file for some examples of phrases and words in the source language vocabulary. When encoding an input Mandarin Chinese sequence into ``pieces'' in the vocabulary, the tokenizer maps the sequence to a series of vocabulary items, each consisting of one or more characters (thanks to the {\monofam{sentencepiece}} tokenizer, we can perform this segmentation even when the original text has no white space). Given this information, how could adding a 1D Convolutional layer after the embedding layer and before passing the embeddings into the bidirectional encoder help our NMT system? \textbf{Hint:} each Mandarin Chinese character is either an entire word or a morpheme in a word. Look up the meanings of 电, 脑, and 电脑 separately for an example. The characters 电 (electricity) and  脑 (brain) when combined into the phrase 电脑 mean computer.

    \ifans{Adding a 1D Convolutional layer after the embedding layer and before passing the embeddings into the bidirectional encoder could help the NMT system by allowing the model to capture local dependencies and patterns among the character sequences. Since each Mandarin Chinese character is either an entire word or a morpheme in a word, the convolutional layer can identify these patterns and use them to inform the representation of the word or phrase as a whole. For example, in the case of the characters 电, 脑, and 电脑, the convolutional layer could potentially learn to recognize the pattern of the characters 电 (electricity) and 脑 (brain) occurring together to form the word 电脑 (computer). By capturing these patterns, the model may be able to improve its ability to handle rare or unseen words, which is important for NMT systems since they must be able to translate sentences containing previously unseen vocabulary.}

    \part[8] Here we present a series of errors we found in the outputs of our NMT model (which is the same as the one you just trained). For each example of a reference (i.e., `gold') English translation, and NMT (i.e., `model') English translation, please:
    
    \begin{enumerate}
        \item Identify the error in the NMT translation.
        \item Provide possible reason(s) why the model may have made the error (either due to a specific linguistic construct or a specific model limitation).
        \item Describe one possible way we might alter the NMT system to fix the observed error. There are more than one possible fixes for an error. For example, it could be tweaking the size of the hidden layers or changing the attention mechanism.
    \end{enumerate}
    
    Below are the translations that you should analyze as described above. Only analyze the underlined error in each sentence. Rest assured that you don't need to know Mandarin to answer these questions. You just need to know English! If, however, you would like some additional color on the source sentences, feel free to use a resource like \url{https://www.archchinese.com/chinese_english_dictionary.html} to look up words. Feel free to search the training data file to have a better sense of how often certain characters occur.

    \begin{subparts}
        \subpart[2]
        \textbf{Source Sentence:} 贼人其后被警方拘捕及被判处盗窃罪名成立。 \newline
        \textbf{Reference Translation:} \textit{\underline{the culprits were} subsequently arrested and convicted.}\newline
        \textbf{NMT Translation:} \textit{\underline{the culprit was} subsequently arrested and sentenced to theft.}
        
        \ifans{
        \begin{enumerate}
            \item The error in the NMT translation is the use of singular form "culprit" instead of its plural form "culprits".
            
            \item The model might have made this error because of a lack of attention to the plural form of the noun "culprits" in the source sentences (Mandarin). Additionally, the model might be trained on a dataset that the frequency of the singular form "culprit" is higher than the plural form.

            \item A possible way to address this error is to increase the weight of the attention mechanism on the number of nouns in the source sentence or to increase the occurrence of plural words in our dataset.
        \end{enumerate}
        }


        \subpart[2]
        \textbf{Source Sentence}: 几乎已经没有地方容纳这些人,资源已经用尽。\newline
        \textbf{Reference Translation}: \textit{there is almost no space to accommodate these people, and resources have run out.   }\newline
        \textbf{NMT Translation}: \textit{the resources have been exhausted and \underline{resources have been exhausted}.}
        
        \ifans{\begin{enumerate}
            \item The error in the NMT translation is the repetition of the phrase "resources have been exhausted".

            \item One possible reason for this error is that the NMT system didn't capture the meaning of the word "space" or "accommodate", leads to the inaccurate weights of attention in the source sentence while trying to translate the first part of the sentence ("there is almost no space to accommodate these people").

            \item This error can be solved by adjusting the attention mechanism to better capture the meaning of sentence. Another the way to do so is to increase the amount of training data which helps improve the accuracy of the translation.
        \end{enumerate}}

        \subpart[2]
        \textbf{Source Sentence}: 当局已经宣布今天是国殇日。 \newline
        \textbf{Reference Translation}: \textit{authorities have announced \underline{a national mourning today.}}\newline
        \textbf{NMT Translation}: \textit{the administration has announced \underline{today's day.}}
        
        \ifans{\begin{enumerate}
            \item The error of the NMT translation is that it misses the meaning of 国殇日 ("national mourning day") and mistranslates it as "today's day".

            \item The model may not have learned the specific translation of "国殇日" as it is a culturally specific term, and may have relied on the literal translation of each individual character.

            \item The model may benefit from being trained on a larger corpus of text that includes culturally specific terms and phrases. Additionally, the model could be improved by incorporating additional context and domain-specific knowledge during the training process, such as incorporating knowledge of national holidays and events.
        \end{enumerate}}
        
        \subpart[2] 
        \textbf{Source Sentence\footnote{This is a Cantonese sentence! The data used in this assignment comes from GALE Phase 3, which is a compilation of news written in simplified Chinese from various sources scraped from the internet along with their translations. For more details, see \url{https://catalog.ldc.upenn.edu/LDC2017T02}. }:} 俗语有云:``唔做唔错"。\newline
        \textbf{Reference Translation:} \textit{\underline{`` act not, err not "}, so a saying goes.}\newline
        \textbf{NMT Translation:} \textit{as the saying goes, \underline{`` it's not wrong. "}}
        
        \ifans{\begin{enumerate}
            \item The error is that the NMT translation is missing the first half of the reference translation, which is the translation of the Chinese idiom.

            \item One possible reason for this error is the shortage of idiom phrases in the training data, which make model to have difficulty understanding idiomatic expressions, as well as the structure of the Chinese language.

            \item To help the model better understand idiomatic expressions, we could provide it a larger training set that includes more diverse examples of idioms and their translations. Additionally, we could explore incorporating a pre-trained language model to improve its understanding of the structure of Chinese language.
        \end{enumerate}}
    \end{subparts}


    \part[14] BLEU score is the most commonly used automatic evaluation metric for NMT systems. It is usually calculated across the entire test set, but here we will consider BLEU defined for a single example.\footnote{This definition of sentence-level BLEU score matches the \texttt{sentence\_bleu()} function in the \texttt{nltk} Python package. Note that the NLTK function is sensitive to capitalization. In this question, all text is lowercased, so capitalization is irrelevant. \\ \url{http://www.nltk.org/api/nltk.translate.html\#nltk.translate.bleu_score.sentence_bleu}
    } 
    Suppose we have a source sentence $\bs$, a set of $k$ reference translations $\br_1,\dots,\br_k$, and a candidate translation $\bc$. To compute the BLEU score of $\bc$, we first compute the \textit{modified $n$-gram precision} $p_n$ of $\bc$, for each of $n=1,2,3,4$, where $n$ is the $n$ in \href{https://en.wikipedia.org/wiki/N-gram}{n-gram}:
    \begin{align}
        p_n = \frac{ \displaystyle \sum_{\text{ngram} \in \bc} \min \bigg( \max_{i=1,\dots,k} \text{Count}_{\br_i}(\text{ngram}), \enspace \text{Count}_{\bc}(\text{ngram}) \bigg) }{\displaystyle \sum_{\text{ngram}\in \bc} \text{Count}_{\bc}(\text{ngram})}
    \end{align}
     Here, for each of the $n$-grams that appear in the candidate translation $\bc$, we count the maximum number of times it appears in any one reference translation, capped by the number of times it appears in $\bc$ (this is the numerator). We divide this by the number of $n$-grams in $\bc$ (denominator). \newline 

    Next, we compute the \textit{brevity penalty} BP. Let $len(c)$ be the length of $\bc$ and let $len(r)$ be the length of the reference translation that is closest to $len(c)$ (in the case of two equally-close reference translation lengths, choose $len(r)$ as the shorter one). 
    \begin{align}
        BP = 
        \begin{cases}
            1 & \text{if } len(c) \ge len(r) \\
            \exp \big( 1 - \frac{len(r)}{len(c)} \big) & \text{otherwise}
        \end{cases}
    \end{align}
    Lastly, the BLEU score for candidate $\bc$ with respect to $\br_1,\dots,\br_k$ is:
    \begin{align}
        BLEU = BP \times \exp \Big( \sum_{n=1}^4 \lambda_n \log p_n \Big)
    \end{align}
    where $\lambda_1,\lambda_2,\lambda_3,\lambda_4$ are weights that sum to 1. The $\log$ here is natural log.
    \newline
    \begin{subparts}
        \subpart[5] Please consider this example: \newline
        Source Sentence $\bs$: \textbf{需要有充足和可预测的资源。} 
        \newline
        Reference Translation $\br_1$: \textit{resources have to be sufficient and they have to be predictable}
        \newline
        Reference Translation $\br_2$: \textit{adequate and predictable resources are required}
        
        NMT Translation $\bc_1$: there is a need for adequate and predictable resources
        
        NMT Translation $\bc_2$: resources be suﬀicient and predictable to
        
        Please compute the BLEU scores for $\bc_1$ and $\bc_2$. Let $\lambda_i=0.5$ for $i\in\{1,2\}$ and $\lambda_i=0$ for $i\in\{3,4\}$ (\textbf{this means we ignore 3-grams and 4-grams}, i.e., don't compute $p_3$ or $p_4$). When computing BLEU scores, show your work (i.e., show your computed values for $p_1$, $p_2$, $len(c)$, $len(r)$ and $BP$). Note that the BLEU scores can be expressed between 0 and 1 or between 0 and 100. The code is using the 0 to 100 scale while in this question we are using the \textbf{0 to 1} scale. Please round your responses to 3 decimal places. 
        \newline
        
        Which of the two NMT translations is considered the better translation according to the BLEU Score? Do you agree that it is the better translation?
        
        \ifans{
        \begin{enumerate}
        \item BLEU score for \textbf{c_1}

        We will first compute the \textit{modified n-gram precision $p_1$ and $p_2$} of $\bc_1$:
        $$p_1 = \frac{0 + 0 + 0 + 0 + 0 + 1 + 1 + 1 + 1}{9} = \frac{4}{9}$$
        
        $$p_2 = \frac{0 + 0 + 0 + 0 + 0 + 1 + 1 + 1}{8} = \frac{3}{8}$$

        We then compute the length of candidate translation $len(c)$ and the length of the reference translation $len(r)$ that is closest to $len(c)$ ($len(r)$ in this case is the first Reference Translation $\br_1$):
        $$len(c_1) = 9$$
        $$len(r_1) = 11$$

        Next, we compute the \textit{brevity penalty} BP:
        $$BP = \exp\left( 1 - \frac{len(r)}{len(c)} \right) = \exp\left( 1 - \frac{11}{9} \right) = e^{-\frac{2}{9}}$$

        Finally, the BLEU score for candidate $\bc_1$ with respect $\br_1, \br_2$ is:
        $$BLEU = BP \times \exp \Big( \sum_{n=1}^2 \lambda_n \log p_n \Big) = \exp\left({-\frac{2}{9} + 0.5 \times (\log \frac{4}{9} + \log \frac{3}{8})}\right) \approx 0.327$$

        \item We will compute the BLEU score for $\bc_2$ similar to $\bc_1$:
        $$p_1 = \frac{1 + 1 + 1 + 1 + 1 + 1}{6} = 1$$

        $$p_2 = \frac{0 + 1 + 1 + 1 + 0}{5} = \frac{3}{5}$$

        $$len(c_2) = 6$$
        $$len(r_2) = 6$$
        
        $$BP = 1 \text{ (because $len(c) = len(r)$)}$$

        $$BLEU = BP \times \exp \Big( \sum_{n=1}^2 \lambda_n \log p_n \Big) = \exp \left(0.5 \times (\log 1 + \log \frac{3}{5}) \right) \approx 0.775$$
        \end{enumerate}

        According to the BLEU score for $\bc_1$ and $\bc_2$, the second NMT translation $\bc_2$ is considered the better translation. However, I would not agree that $\bc_2$ is translated well compared to $\bc_1$.
        }
        
        \subpart[5] Our hard drive was corrupted and we lost Reference Translation $\br_1$. Please recompute BLEU scores for $\bc_1$ and $\bc_2$, this time with respect to $\br_2$ only. Which of the two NMT translations now receives the higher BLEU score? Do you agree that it is the better translation?
        
        \ifans{
        \begin{enumerate}
            \item BLEU score for $\bc_1$ with respect to $\br_2$:
            $$p_1 = \frac{0 + 0 + 0 + 0 + 0 + 1 + 1 + 1 + 1}{9} = \frac{4}{9}$$

            $$p_2 = \frac{0 + 0 + 0 + 0 + 0 + 1 + 1 + 1}{8} = \frac{3}{8}$$
    
            $$len(c_1) = 9$$
            $$len(r_2) = 6$$
            
            $$BP = 1 \text{ (because $len(c) > len(r)$)}$$
    
            $$BLEU = BP \times \exp \Big( \sum_{n=1}^2 \lambda_n \log p_n \Big) = \exp \left(0.5 \times (\log \frac{4}{9} + \log \frac{3}{8}) \right) \approx 0.408$$

            \item BLEU score for $\bc_2$ with respect to $\br_2$:
            $$p_1 = \frac{1 + 0 + 0 + 1 + 1 + 0}{6} = \frac{1}{2}$$

            $$p_2 = \frac{0 + 0 + 0 + 1 + 0}{5} = \frac{1}{5}$$
    
            $$len(c_1) = 6$$
            $$len(r_2) = 6$$
            
            $$BP = 1 \text{ (because $len(c) = len(r)$)}$$
    
            $$BLEU = BP \times \exp \Big( \sum_{n=1}^2 \lambda_n \log p_n \Big) = \exp \left(0.5 \times (\log \frac{1}{2} + \log \frac{1}{5}) \right) \approx 0.316$$

            The first translation $\bc_1$ now has a higher BLEU score, which is reasonable as $\bc_1$ seems to be the better translation.
        \end{enumerate}
        }
        
        \subpart[2] Due to data availability, NMT systems are often evaluated with respect to only a single reference translation. Please explain (in a few sentences) why this may be problematic. In your explanation, discuss how the BLEU score metric assesses the quality of NMT translations when there are multiple reference translations versus a single reference translation.
        
        \ifans{Translations from a source language can vary a lot due to the flexibility of the target language, e.g. using synonyms, antonyms... NMT systems being evaluated with respect to only a single reference translation can ignore the variations, resulting in it being given low BLEU score even though it is a high quality translation. With multiple reference translations, the BLEU score metric can be more reliable and accurate as it can cover different variations of the translated sentences. On the contrary, the BLEU score can vary widely and does not give an accurate score when assessing the quality of the NMT translations with respect to a single reference translation. However, in the original BLEU paper, they stated that we may use a big test corpus with a single reference translation, provided that the translations are not all from the same translator.}
        
        \subpart[2] List two advantages and two disadvantages of BLEU, compared to human evaluation, as an evaluation metric for Machine Translation. 
        
        \ifans{\\
        \textbf{Advantages:}
        \begin{enumerate}
            \item BLEU is an automatic evaluation metric that is quicker and inexpensive compared to human evaluation, which can take weeks or months to finish and involve human labor that can not be reused.

            \item BLEU score is language-independent, i.e. reliable with different source - target translation and shows a significantly high correlation with human judgements.
        \end{enumerate}

        \textbf{Disadvantages:}
        \begin{enumerate}
            \item BLEU metric neither consider the meanings of the word nor understands the significance of the words in the context. For example, the propositions usually have the lowest level of importance. However, BLEU sees them as important as nouns and verb keywords.

            \item BLEU doesn't understand the variants of the words and can't take the word order into account.
        \end{enumerate}
        }
        
    \end{subparts}
\end{parts}
