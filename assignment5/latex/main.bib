@article{radford2018improving,
  title={Improving language understanding with unsupervised learning},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={Technical report, OpenAI},
  year={2018}
}

@article{raffel2020exploring,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@article{tay2020synthesizer,
  title={Synthesizer: Rethinking self-attention in transformer models},
  author={Tay, Yi and Bahri, Dara and Metzler, Donald and Juan, Da-Cheng and Zhao, Zhe and Zheng, Che},
  journal={arXiv preprint arXiv:2005.00743},
  year={2020}
}

@inproceedings{hawthorne2022perceiver,
  author    = {Curtis Hawthorne and
               Andrew Jaegle and
               Catalina Cangea and
               Sebastian Borgeaud and
               Charlie Nash and
               Mateusz Malinowski and
               Sander Dieleman and
               Oriol Vinyals and
               Matthew M. Botvinick and
               Ian Simon and
               Hannah Sheahan and
               Neil Zeghidour and
               Jean{-}Baptiste Alayrac and
               Jo{\~{a}}o Carreira and
               Jesse H. Engel},
  title     = {General-purpose, long-context autoregressive modeling with Perceiver
               {AR}},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {8535--8558},
  year      = {2022},
}